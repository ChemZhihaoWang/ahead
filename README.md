# AHEAD: Automated platform for high-efficiency electrolyzer adaptive channel design 

## Introduction
AHEAD (Automated platform for high-efficiency electrolyzer adaptive channel design) is an innovative platform that synergistically integrates multiple advanced technologies for systematic optimization of electrolyzer flow channels. The platform combines expert-guided design, multi-physics simulations, machine learning model predictions, interpretable feature analysis, stochastic structure generation, and experimental validation through 3D printing and high-speed imaging. This comprehensive approach enables efficient and automated design of high-performance electrolyzer channels.

This codebase is developed for the paper "Machine learning-driven discovery of optimal designs for water electrolysis devices".

## Structure
```
.
├── config/                # Configuration files directory
├── config_*.yaml          # Model-specific configuration files
├── dataset/               # Dataset storage directory
├── checkpoint/            # Model checkpoints and saved weights
├── experiments/           # Experiment results and logs
├── explainability/        # Model explainability related code
├── models/                # Model definitions
├── scripts/               # Utility scripts
├── utils/                 # Utility functions
├── main.py                # Main program entry
└── environment.yml        # Conda environment configuration file
```

## Installation
1. Clone the repository:
```bash
git clone https://github.com/ChemZhihaoWang/hydro_channel.git
cd hydro_channel
```

2. Create and activate Conda environment:
```bash
conda env create -f environment.yml
conda activate ahead
```

3. Configure Local Paths

Before running the code, you need to modify the following path configurations:

- In the configuration files under `config/` directory, update the dataset paths, model save paths, etc. to your local paths
- In `main.py`, ensure all file paths point to the correct local locations
- In the data processing scripts under `dataset/` directory, update the local dataset storage paths

Example of path configuration in YAML files:
```yaml
data:
  root_dir: "/path/to/your/local/dataset"
  save_dir: "/path/to/your/local/save/directory"
```

## Dataset Download

The datasets are stored on Google Drive and can be downloaded using the provided script. Available datasets include:

- `sampling_200`: Sampling dataset with 200 samples
- `sampling_400`: Sampling dataset with 400 samples
- `sampling_600`: Sampling dataset with 600 samples
- `sampling_800`: Sampling dataset with 800 samples
- `hydrofig`: All dataset with 1000 samples
- `SISSO`: SISSO analysis dataset
- `test_images`: Test images dataset for `sampling_800`
- `label`: Label dataset for figure datasets (`sampling_200`–`hydrofig`).
- `correlation`: Correlation analysis dataset
- `randomfig_100w`: One million new channel structure generated by LDL-RW algorithm.
    
    > Note: Due to the large file size (3.09GB), direct download may fail. If needed, you can download it manually from: https://drive.google.com/file/d/1kXxXIs_uT4msnK317y8V1clvTv-2oMAd/view?usp=drive_link

To download a dataset:

```bash
# Download a specific dataset
python dataset/download_data.py --dataset hydrofig

# Download with custom save directory
python dataset/download_data.py --dataset hydrofig --data-dir "/path/to/save"

# Keep the downloaded archive file
python dataset/download_data.py --dataset test_images --keep-archive
```

The script will:
1. Download the dataset from Google Drive
2. Extract the contents to the specified directory
3. Remove the archive file by default (use `--keep-archive` to keep it)

## Usage Instructions

### Model Training and Evaluation
To run training and evaluation:
```bash
python main.py --config config_*.yaml
nohup python main.py --config config_*.yaml > output_*.log 2>&1 &
```

### Supported Models

- **Residual Network (ResNet)**: A deep convolutional neural network architecture that introduced residual connections to address the vanishing gradient problem in deep networks. ResNet's skip connections enable training of very deep networks (hundreds of layers) while maintaining good performance. It's particularly effective for feature extraction from flow field images and pattern recognition in channel designs.

- **Vision Transformer (ViT)**: A transformer-based architecture for computer vision tasks that processes images as sequences of patches. ViT achieves state-of-the-art performance by applying self-attention mechanisms to image patches, enabling global context modeling and long-range dependencies. 

- **Swin Transformer (Swin-T)** : A hierarchical vision transformer that introduces shifted windows to reduce computational complexity while maintaining the benefits of global attention. Swin Transformer's hierarchical structure makes it suitable for multi-scale feature learning, which is crucial for analyzing flow patterns at different scales in electrolyzer channels.

- **Graph Isomorphism Network (GIN)**: A powerful graph neural network architecture designed for graph classification and node classification tasks. GIN uses a simple but effective message passing scheme that can distinguish between different graph structures. 

- **Mixture of Experts (MoE)**: A neural network architecture that consists of multiple expert networks and a gating network. The gating network dynamically routes inputs to different experts, allowing the model to specialize in different aspects of the data. This architecture is especially useful for handling diverse flow patterns and complex physical phenomena in electrolyzer channels.

## License
This project is licensed under the MIT License. See the LICENSE file for details.
